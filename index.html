<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yanchao Yang</title>
    <!-- Include Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Raleway:wght@400;700&display=swap" rel="stylesheet">
    <!-- Include Font Awesome for social media icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <!-- Include external CSS file -->
    <link rel="stylesheet" href="./styles.css">
</head>

<body>
    <header>
        <h1>InfoBodied AI Lab</h1>
    </header>
    <nav>
        <a href="#about-me">About Me</a>
        <a href="#news">News</a>
        <a href="#publications">Publications</a>
        <a href="#members">Members</a>
        <a href="#professional-service">Professional Service</a>
    </nav>
    <div class="profile">
        <img src="./yanchaoy/yanchao_pic.jpg" alt="Yanchao Yang">
        <div class="profile-info">
            <h1>Yanchao Yang</h1>
            <b>Assistant Professor</b><br>
            <a href="https://www.eee.hku.hk/">Electrical and Computer Engineering</a> and the <a href="https://datascience.hku.hk/">Institute of Data Science</a><br>
            <a href="https://www.hku.hk/">The University of Hong Kong</a><br>
            <b>Email:</b> <tt>yanchaoy at hku dot hk</tt><br>
            <b>Office:</b> Room 714, Chow Yei Ching Building, HKU<br>
            <div class="social-icons">
                <a href="https://scholar.google.com/citations?user=r2tKnV4AAAAJ&hl=en" target="_blank"><i class="fab fa-google"></i></a>
                <a href="https://github.com/YanchaoYang" target="_blank"><i class="fab fa-github"></i></a>
                <a href="https://www.linkedin.com/in/yanchao-yang-b95b13ba/" target="_blank"><i class="fab fa-linkedin"></i></a>
            </div>
        </div>
    </div>

    <div class="container">

        <section id="about-me">
            <h2>About Me</h2>
            <div class="section">
                <ul>
                    I am an Assistant Professor at HKU, jointly appointed by the Department of
                    <a href="https://www.eee.hku.hk/"><span style="font-weight: normal;">Electrical and Computer Engineering (ECE)</span></a> and the HKU Musketeers Foundation
                    <a href="https://datascience.hku.hk/"><span style="font-weight: normal;">Institute of Data Science (HKU-IDS)</span></a>. I was a Postdoctoral Research Fellow at
                    <a href="https://www.stanford.edu/"><span style="font-weight: normal;">Stanford University</span></a> with
                    <a href="https://scholar.google.com/citations?user=5JlEyTAAAAAJ&hl=en"><span style="font-weight: normal;">Leonidas J. Guibas</span></a> and received my Ph.D. from the
                    <a href="https://www.ucla.edu/"><span style="font-weight: normal;">University of California, Los Angeles (UCLA)</span></a> with
                    <a href="https://scholar.google.com/citations?hl=en&user=lH1PdF8AAAAJ"><span style="font-weight: normal;">Stefano Soatto</span></a>. Earlier, I obtained my Master's and Bachelor's degrees from
                    <a href="https://www.kaust.edu.sa/en"><span style="font-weight: normal;">KAUST</span></a> and
                    <a href="https://en.ustc.edu.cn/"><span style="font-weight: normal;">USTC</span></a>, respectively.
                    <br>
                    <br>
                    We do research in <b>embodied AI</b> and are interested in self-/semi-supervised techniques that allow embodied agents to <b>learn at low-annotation regimes</b>. Our long-term goal is to design learning algorithms that enable embodied agents to continuously build scene representations and acquire interaction skills through active perception with multimodal signals. Our recent effort is to develop efficient mutual information estimators and automate the learning of perception, compositional scene representation, and interaction policy for embodied intelligence in open world, namely, <b>InfoBodied AI</b>. We are also grounding Large Foundation Models to the physical world via information-theoretic tools.
                    <br>
                    <br>
                    <mark class="custom-mark">Ph.D. students, postdocs and interns!</mark>
                    <br>
                    <br> 
                    We are constantly looking for talents with strong motivation to build fundamentals for autonomous agents to learn from unlimited data streams toward intelligent interactions with the physical world. This quest is related but not limited to computer vision and graphics, machine learning, communication and information theory, multimodal data mining, robotics, and human-machine interaction. Please contact me via <b>email</b> if you are interested in our research or potential collaborations.
                    <br>
                    <br>
                    *Students can choose either ECE or Data Science when applying, but make sure to notify me after the submission.
                    <br>
                    <br>
                    *Strong Ph.D. candidates are welcome to apply for
                    <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/hong_kong_phd_fellowship_scheme">HKPFS</a> and
                    <a href="https://gradsch.hku.hk/prospective_students/fees_scholarships_and_financial_support/hku_presidential_phd_scholar_programme">HKUPS</a> (all year around).
                    <br>
                    <br>
                    <!--
                    *We are now accepting summer interns via the HKU Summer Research Programme (<a href="https://gradsch.hku.hk/news_and_events/news_and_future_events/summer-research-programme-2024">SRP</a>), deadline is Jan. 26, 2024. Please apply asap!
                    <br><br>
                    -->
                    *We are now considering applications in the
                    <a href="https://gradsch.hku.hk/prospective_students/application/application_period">Main Round for 2025/26</a>, deadline is Dec. 1, 2024. Please apply asap!
                </ul>
            </div>
        </section>

        <section id="news">
            <h2>News</h2>
            <div class="section">
                <ul>
                    <li> [10/2024] Pei Zhou just gave an <a href="https://eccv2024.ecva.net/virtual/2024/events/oral">Oral talk</a> at ECCV on manipulation concept discovery (<a href="https://arxiv.org/abs/2407.15086">MaxMI</a>). Congrats!</li>
                    <li> [09/2024] I am serving as an Area Chair for the IEEE/CVF Computer Vision and Pattern Recognition Conference (<a href="https://cvpr.thecvf.com/">CVPR 2025</a>).</li>
                    <li> [07/2024] Zhengyang Hu just gave an <a href="https://icml.cc/virtual/2024/oral/35575">Oral talk</a> at ICML on neural estimation of mutual information (<a href="https://openreview.net/pdf/1aa6b6cda6db8f87371ef8b871fc99a7fdd620b3.pdf">InfoNet</a>). Congrats!</li>
                    <li> [07/2024] Thanks for the Early Career Award from the Research Grants Council (<a href="https://awards.ugc.edu.hk/">RGC</a>). Also thank the committee and the reviewers for their insightful comments!</li>
                    <li> [06/2024] I will be teaching with Prof. Yi Ma in the summer school on <a href="https://datascience.hku.hk/2024/08/reaching-a-more-profound-understanding-of-ai-research-with-hku-ids-summer-course-idss-2402-towards-ai-by-deep-neural-networks/">Towards AI by Deep Neural Networks</a>.</li>
                    <li> [03/2024] I am serving as an Area Chair for the European Conference on Computer Vision (<a href="https://eccv.ecva.net/">ECCV 2024</a>).</li>
                    <li> [01/2024] Our conference on Parsimony and Learning <a href="https://2024.cpal.cc/">CPAL</a> is successfully concluded.</li>
                </ul>
            </div>
        </section>

        <section id="publications">
            <h2>Publications</h2>
            See <a href="https://scholar.google.com/citations?user=r2tKnV4AAAAJ&hl=en">Google Scholar</a> for a full list of papers. *: equal contributions, <sup>†</sup>: corresponding author
            <br>
            <br>

            <div class="publication">
                <div class="image-container">
                <img src="./yanchaoy/cigtime.png" alt="cigtime">
                </div>
                <div>
                    <p>
                    <b>
                    CigTime: Corrective Instruction Generation Through Inverse Motion Editing
                    </b>
                    <br>
                    <br>
                    Qihang Fang, Chengcheng Tang, Bugra Tekin, <b>Yanchao Yang</b>
                    <br>
                    <br> 
                    NeurIPS 2024
                    <br>
                    <br>
                    <a href="">arXiv</a>/<a href="">code</a>/<a href="">project page</a>
                    </p>
                </div>
            </div>

            <div class="publication">
                <div class="image-container">
                <img src="./yanchaoy/clover.jpg" alt="clover">
                </div>
                <div>
                    <p>
                    <b>
                    Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation
                    </b>
                    <br>
                    <br>
                    Qingwen Bu*, Jia Zeng*, Li Chen*, <b>Yanchao Yang</b><sup>†</sup>, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, Hongyang Li<sup>†</sup>
                    <br>
                    <br>
                    NeurIPS 2024
                    <br>
                    <br>
                    <a href="https://arxiv.org/abs/2409.09016">arXiv</a>/<a href="https://github.com/OpenDriveLab/CLOVER">code</a>/<a href="https://github.com/OpenDriveLab/CLOVER">project page</a>
                    </p>
                </div>
            </div>

            <div class="publication">
                <div class="image-container">
                <img src="./yanchaoy/maxmi.png" alt="maxmi">
                </div>
                <div>
                    <p>
                    <b>
                    MaxMI: A Maximal Mutual Information Criterion for Manipulation Concept Discovery
                    </b>
                    <br>
                    <br>
                    Pei Zhou, <b>Yanchao Yang</b>
                    <br>
                    <br>
                    ECCV 2024 Oral
                    <br>
                    <br>
                    <a href="https://arxiv.org/abs/2407.15086">arXiv</a>/<a href="https://github.com/PeiZhou26/MaxMI">code</a>/<a href="https://github.com/PeiZhou26/MaxMI">project page</a>
                    </p>
                </div>
            </div>

            <div class="publication">
                <div class="image-container">
                <img src="./yanchaoy/infonorm.png" alt="infonorm">
                </div>
                <div>
                    <p>
                    <b>
                    InfoNorm: Mutual Information Shaping of Normals for Sparse-View Reconstruction
                    </b>
                    <br>
                    <br>
                    Xulong Wang*, Siyan Dong*, Youyi Zheng, <b>Yanchao Yang</b>
                    <br>
                    <br>
                    ECCV 2024
                    <br>
                    <br>
                    <a href="https://arxiv.org/abs/2407.12661">arXiv</a>/<a href="https://github.com/Muliphein/InfoNorm">code</a>/<a href="https://muliphein.github.io/InfoNorm/">project page</a>
                    </p>
                </div>
            </div>

            <div class="publication">
                <div class="image-container">
                <img src="./yanchaoy/sgnerf.png" alt="sgnerf">
                </div>
                <div>
                    <p>
                    <b>
                    SG-NeRF: Neural Surface Reconstruction with Scene Graph Optimization
                    </b>
                    <br>
                    <br>
                    Yiyang Chen*, Siyan Dong*, Xulong Wang, Lulu Cai, Youyi Zheng, <b>Yanchao Yang</b>
                    <br>
                    <br>
                    ECCV 2024
                    <br>
                    <br>
                    <a href="https://arxiv.org/abs/2407.12667">arXiv</a>/<a href="https://github.com/Iris-cyy/SG-NeRF">code</a>/<a href="https://github.com/Iris-cyy/SG-NeRF">project page</a>
                    </p>
                </div>
            </div>

            <div class="publication">
                <div class="image-container">
                <img src="./yanchaoy/disco.png" alt="disco">
                </div>
                <div>
                    <p>
                    <b>
                    DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control
                    </b>
                    <br>
                    <br>
                    Xinyu Xu, Shengcheng Luo, <b>Yanchao Yang</b>, Yong-Lu Li, Cewu Lu
                    <br>
                    <br>
                    ECCV 2024
                    <br>
                    <br>
                    <a href="https://arxiv.org/abs/2407.14758">arXiv</a>/<a href="https://github.com/AllenXuuu/DISCO">code</a>/<a href="https://github.com/AllenXuuu/DISCO">project page</a>
                    </p>
                </div>
            </div>

            <div class="publication">
                <div class="image-container">
                <img src="./yanchaoy/revisit.png" alt="revisit">
                </div>
                <div>
                    <p>
                    <b>
                    Revisit Human-Scene Interaction via Space Occupancy
                    </b>
                    <br>
                    <br>
                    Xinpeng Liu, Haowen Hou, <b>Yanchao Yang</b>, Yong-Lu Li, Cewu Lu
                    <br>
                    <br>
                    ECCV 2024
                    <br>
                    <br>
                    <a href="https://arxiv.org/abs/2312.02700">arXiv</a>/<a href="https://github.com/HaowenHou/Motion-Occupancy-Base">code</a>/<a href="https://foruck.github.io/occu-page/">project page</a>
                    </p>
                </div>
            </div>

            <div class="publication">
                <div class="image-container">
                <img src="./yanchaoy/infonet.png" alt="infonet">
                </div>
                <div>
                    <p>
                    <b>
                    InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization
                    </b>
                    <br>
                    <br>
                    Zhengyang Hu, Song Kang, Qunsong Zeng, Kaibin Huang, <b>Yanchao Yang</b>
                    <br>
                    <br>
                    ICML 2024 Oral
                    <br>
                    <br>
                    <a href="https://arxiv.org/abs/2402.10158">arXiv</a>/<a href="https://github.com/datou30/InfoNet">code</a>/<a href="https://datou30.github.io/InfoNet-page/">project page</a>
                    </p>
                </div>
            </div>

            <div class="publication">
                <div class="image-container">
                <img src="./yanchaoy/infocon.png" alt="infocon">
                </div>
                <div>
                    <p>
                    <b>
                    InfoCon: Concept Discovery with Generative and Discriminative Informativeness
                    </b>
                    <br>
                    <br>
                    Ruizhe Liu, Qian Luo, <b>Yanchao Yang</b>
                    <br>
                    <br>
                    ICLR 2024
                    <br>
                    <br>
                    <a href="https://arxiv.org/abs/2404.10606">arXiv</a>/<a href="https://github.com/zrllrz/InfoCon">code</a>/<a href="https://zrllrz.github.io/InfoCon_/">project page</a>
                    </p>
                </div>
            </div>

            <div class="publication">
                <div class="image-container">
                <img src="./yanchaoy/t2r.png" alt="t2r">
                </div>
                <div>
                    <p>
                    <b>
                    Text2Reward: Reward Shaping with Language Models for Reinforcement Learning
                    </b>
                    <br>
                    <br>
                    Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, <b>Yanchao Yang</b><sup>†</sup>, Tao Yu<sup>†</sup>
                    <br>
                    <br>
                    ICLR 2024 Spotlight
                    <br>
                    <br>
                    <a href="https://arxiv.org/abs/2309.11489">arXiv</a>/<a href="https://github.com/xlang-ai/text2reward">code</a>/<a href="https://text-to-reward.github.io/">project page</a>
                    </p>
                </div>
            </div>

        </section>

        <section id="members">
            <h2>Members</h2>
            <ul>
            <li> Current: </li>
                 <ul>
                     <li> Postdocs: </li>
                         <ul>
                             <li> Siyan Dong (Ph.D. Shandong University) </li>
                         </ul>
                     <li> Students: </li>
                         <ul>
                             <li> Lulu Cai (B.S. Zhejiang University) </li>
                             <li> Ruizhe Liu (B.S. Peking University) </li>
                             <li> Zewen Wu (M.S. B.S. Xi'an Jiao Tong University Gifted Young) </li>
                             <li> Jinghan Yang (B.S. HKU) </li>
                             <li> Pei Zhou (M.S. UCSD, B.S. Fudan University) </li>
                             <li> Anupam Pani (B.S. CityU) </li>
                             <li> Li Sun (B.S. Zhejiang University) </li>
                             <li> Qian Luo (M.S. Georgia Tech, B.S. HUST) </li>
                             <li> Qihang Fang (M.S. B.S. Shandong University) </li>
                             <li> Qingyuan Zheng (M.S. HKUST, B.S. SJTU) </li>
                             <li> Yunchao Zhang (B.S. Zhejiang University) </li>
                             <li> Zhengyang Hu (B.S USTC Gifted Young) </li>
                         </ul>
                 </ul>
            <li> Alumni: </li>
                 <ul>
                     <li> Hanyang Chen (B.S. HKU, now UIUC) </li>
                     <li> Xiaomeng Xu (B.S. Tsinghua University, now Stanford) </li>
                     <li> Ruohan Zhang (B.S. USTC, now UIUC) </li>
                     <li> Jiateng Liu (B.S. Zhejiang University, now UIUC) </li>
                     <li> Ruoxi Shi (B.S. SJTU, now UCSD) </li>
                     <li> Yang Zheng (B.S. Tsinghua University, now at Stanford) </li>
                     <li> Yutong Chen (M.S. B.S. Tsinghua University, now ETH Zurich) </li>
                 </ul>
            </ul>
        </section>

        <section id="professional-service">
            <h2>Professional Service</h2>
            <ul>
            <li> Program committee/reviewer: </li>
                <ul>
                    <li> Conferences: CVPR, ICCV, ECCV, ICML, ICLR, IJCAI, AAAI, ICRA, NeurIPS </li>
                    <li> Journals: TPAMI, TIP, IJCV, RAL </li>
                </ul>
            </ul>
        </section>

    </div>

    <footer>
        <p> &copy; 2024 Yanchao Yang. All rights reserved. </p>
        <p> Designed by Chen Zhao. </p>
    </footer>

</body>
</html>
