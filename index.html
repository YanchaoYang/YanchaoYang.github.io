<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<title>Yanchao Yang</title>
<link rel="stylesheet" type="text/css" href="./yanchaoy/main.css">
</head>

<body>

<table>
<tbody><tr>
<td><img src="./yanchaoy/yanchao_pic.jpg" width="220"></td>
<td>
<div style="font-size:38; font-weight:bold">Yanchao Yang</div>
<div>
Postdoctoral Research Fellow<br>
<a href="https://geometry.stanford.edu/">Geometric Computation Group</a> and <a href="https://ai.stanford.edu/">Artificial Intelligence Lab</a><br>
<a href="https://cs.stanford.edu/">Computer Science</a>, Stanford University<br>
<br>

<b>Email:</b> <tt>yanchaoy at cs dot stanford dot edu</tt><br>
<b>Office:</b> S256 James H. Clark Center<br>
<a href="https://scholar.google.com/citations?user=r2tKnV4AAAAJ&hl=en">[Google Scholar]</a><a href="https://github.com/YanchaoYang">[Github]</a><a href="https://www.linkedin.com/in/yanchao-yang-b95b13ba/">[LinkedIn]</a>

<br>

</div>
</td>
</tr>
</tbody></table>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>




<div class="section">
<h3>About Me</h3>
<ul>
I am a Postdoctoral Research Fellow at <a href="https://www.stanford.edu/">Stanford University</a>, working with Prof. <a href="https://geometry.stanford.edu/member/guibas/index.html">Leonidas J. Guibas</a> at the <a href="https://geometry.stanford.edu/">Geometric Computation Group</a>. 
Previously, I received my Ph.D. from the <a href="https://www.ucla.edu/">University of California, Los Angeles (UCLA)</a>, advised by Prof. <a href="http://web.cs.ucla.edu/~soatto/">Stefano Soatto</a> at the <a href="http://vision.ucla.edu/">UCLA Vision Lab</a> in 2019.
Earlier, I obtained my Master's and Bachelor's degrees from <a href="https://www.kaust.edu.sa/en">KAUST</a> and <a href="https://en.ustc.edu.cn/">USTC</a> in 2014 and 2011.<br>
<br>
I do research in computer vision, machine learning, and robotics. I am currently interested in unsupervised or self-supervised (2D,3D&4D) representation learning and domain adaptation that help reduce supervision for vision tasks. My long-term research goal is to design learning algorithms that enable autonomous agents to continuously acquire object-centric representations through active perception with multi-modal signals for compositional scene understanding and reasoning in the real world.<br>
<br>
If you are interested in my research or potential collaborations, please contact me via email.
</div>
<br>


<!--<div class="section">
<h3>News</h3>
<ul>

<li> <b style="color: green; background-color: #ffff42">NEW</b> My co-first author paper, <i>CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds</i>, receives ICCV <b><font color='red'>oral presentation</font></b> (acceptance rate: 3%) </b>!
<li> <b style="color: green; background-color: #ffff42">NEW</b> Two papers accepted to ICCV 2021.
<li> <b style="color: green; background-color: #ffff42">NEW</b> I will serve as an area chair (AC) of <a href="http://cvpr2022.thecvf.com">CVPR 2022</a>.</b>
<li> <b style="color: green; background-color: #ffff42">NEW</b> I am serving as an area chair (AC) of <a href="http://wacv2022.thecvf.com">WACV 2022</a>.</b>
<li> <b style="color: green; background-color: #ffff42">NEW</b> I am serving as an executive area chair (EAC) of <a href="http://valser.org">VALSE</a>.</b>
<li> <b style="color: green; background-color: #ffff42">NEW</b> We are organizing the first workshop on <a href="">SEAI: Simulation Technology for Embodied AI</a> at <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>.</b>
<li>  On Apr. 9, I passed my PhD oral exam!</b>
<li>  Three papers accepted to CVPR 2021, two of which receive <b><font color='red'>oral presentations</font></b>!
<li>  My new  work, <i>3DIoUMatch</i>, for semi-supervised 3D detection is accepted by CVPR 2021.
<li>  My new work, <i>MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization</i>, receives CVPR 2021 <b><font color='red'>oral</font></b>.
<li>  My new work, <i>Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments</i>, receives CVPR 2021 <b><font color='red'>oral</font></b>.
<li> Two papers accepted to ECCV 2020.
<li> My first-author paper, <i>Category-level Articulated Object Pose Estimation</i>, gets <b><font color='red'>oral presentation</font></b> in CVPR 2020. 
<li> My co-author paper, <i>SAPIEN: A SimulAted Part-based Interactive ENvironment</i>, also gets <b><font color='red'>oral</font></b> in CVPR 2020.
<li>Two papers accepted to CVPR 2020.
<li>My first-author paper, <i>Learning a Generative Model for Multi-Step Human-Object Interactions from Videos</i>, is awarded <b><font color='red'>Best Paper Honorable Mention</font></b> in Eurographics 2019. The code and data is available in the <a href="https://github.com/hughw19/ActionPlotGeneration">github page</a>. 
<li>My first-author paper, <i>Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation</i>, gets <b><font color='red'>oral presentation</font></b> in CVPR 2019.
<li>Two papers accepted to CVPR 2019.
<li> [Feburary 2019] I receive <b>research intern</b> offer from <a href="https://research.fb.com/category/facebook-ai-research/">Facebook Artificial Intelligence Research (FAIR)</a>. I will work with <a href="https://am.is.tuebingen.mpg.de/person/fmeier">Franziska Meier</a> and <a href="http://geometry.stanford.edu/member/guibas/index.html">Leonidas Guibas</a> starting this June. 
<li> [Feburary 2019] I start my part-time job as a <b>student researcher</b> in Daydream, Google. -->

</ul>
</div>
<br>

<div class="mainsection">

<a name="publications"></a>
<div class="mainsection">
<h3>Publications</h3>
<ul>
See <a href="https://scholar.google.com/citations?user=r2tKnV4AAAAJ&hl=en">Google Scholar</a> for a full list of papers. *: equivalent contribution, <span>&#8224;</span>: corresponding author<br><br>
<table width="100%">

    
<!-- DAGAI -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/DAGAI.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">Domain Adaptation on Point Clouds via Geometry-Aware Implicits</a></b><br><br>Yuefan Shen*, <b>Yanchao Yang*<span>&#8224;</span></b>, Mi Yan, He Wang, Youyi Zheng<span>&#8224;</span>, Leonidas J. Guibas<br><br> CVPR 2022 <br><br>
<a href="https://arxiv.org/abs/2112.09343">arXiv</a>/<a href="javascript:hideshow(document.getElementById('DAGAI'))">bibtex</a>
</p><pre><p id="DAGAI" style="font:18px; display: none">
@article{shen2021domain,
  title={Domain Adaptation on Point Clouds via Geometry-Aware Implicits},
  author={Shen, Yuefan and Yang, Yanchao and Yan, Mi and Wang, He and Zheng, Youyi and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2112.09343},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


    
<!-- ADeLA -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/adela.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation</a></b><br><br><b>Yanchao Yang*<span>&#8224;</span></b>, Hanxiang Ren*, He Wang, Bokui Shen, Qingnan Fan, Youyi Zheng<span>&#8224;</span>, C. Karen Liu, Leonidas J. Guibas<br><br> CVPR 2022 (Oral)<br><br>
<a href="https://arxiv.org/abs/2107.14285">arXiv</a>/<a href="javascript:hideshow(document.getElementById('adela'))">bibtex</a>
</p><pre><p id="adela" style="font:18px; display: none">
@article{yang2021adela,
  title={ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation},
  author={Yang, Yanchao and Ren, Hanxiang and Wang, He and Shen, Bokui and Fan, Qingnan and Zheng, Youyi and Liu, C Karen and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2107.14285},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
    
    
    
<!-- ObjectP -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/objectP.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">Object Pursuit: Building a Space of Objects via Discriminative Weight Generation</a></b><br><br>Chuanyu Pan*, <b>Yanchao Yang*</b>, Kaichun Mo, Yueqi Duan, Leonidas J. Guibas<br><br> ICLR 2022 <br><br>
<a href="https://arxiv.org/abs/2112.07954">arXiv</a>/<a href="javascript:hideshow(document.getElementById('ObjectP'))">bibtex</a>
</p><pre><p id="ObjectP" style="font:18px; display: none">
@article{pan2021object,
  title={Object Pursuit: Building a Space of Objects via Discriminative Weight Generation},
  author={Pan, Chuanyu and Yang, Yanchao and Mo, Kaichun and Duan, Yueqi and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2112.07954},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>



<!-- IFR-Explore -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/IFR_explore.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes</a></b><br><br>Qi Li*, Kaichun Mo*, <b>Yanchao Yang</b>, Hang Zhao, Leonidas J. Guibas<br><br> ICLR 2022 <br><br>
<a href="https://arxiv.org/abs/2112.05298">arXiv</a>/<a href="javascript:hideshow(document.getElementById('IFR-Explore'))">bibtex</a>
</p><pre><p id="IFR-Explore" style="font:18px; display: none">
@article{li2021ifr,
  title={IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes},
  author={Li, Qi and Mo, Kaichun and Yang, Yanchao and Zhao, Hang and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2112.05298},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>



<!-- DCL -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/dcl.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis</a></b><br><br><b>Yanchao Yang*<span>&#8224;</span></b>, Yuefan Shen*, Youyi Zheng<span>&#8224;</span>, C. Karen Liu, Leonidas J. Guibas<br><br> RA-L & ICRA 2022 <br><br>
<a href="https://arxiv.org/abs/2107.13087">arXiv</a>/<a href="javascript:hideshow(document.getElementById('dcl'))">bibtex</a>
</p><pre><p id="dcl" style="font:18px; display: none">
@article{yang2021dcl,
  title={DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis},
  author={Yang, Yanchao and Shen, Yuefan and Zheng, Youyi and Liu, C Karen and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2107.13087},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- DyStaB -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/dystab.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">DyStaB: Unsupervised Object Segmentation via Dynamic-Static Bootstrapping</a></b><br><br><b>Yanchao Yang*</b>, Brian Lai*, Stefano Soatto<br><br> CVPR 2021 (Oral) <br><br>
<a href="https://arxiv.org/abs/2008.07012">arXiv</a>/<a href="https://github.com/blai88/unsupervised_segmentation">Code</a>/<a href="https://www.youtube.com/watch?v=jJM6DVfWNEs">Video</a>/<a href="javascript:hideshow(document.getElementById('dystab'))">bibtex</a>
</p><pre><p id="dystab" style="font:18px; display: none">
@inproceedings{yang2021dystab,
  title={DyStaB: Unsupervised Object Segmentation via Dynamic-Static Bootstrapping},
  author={Yang, Yanchao and Lai, Brian and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2826--2836},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- SADM -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/sadm.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">Learning Semantic-Aware Dynamics for Video Prediction</a></b><br><br>Xinzhu Bei, <b>Yanchao Yang</b>, Stefano Soatto<br><br> CVPR 2021 <br><br>
<a href="https://arxiv.org/abs/2104.09762">arXiv</a>/<a href="javascript:hideshow(document.getElementById('sadm'))">bibtex</a>
</p><pre><p id="sadm" style="font:18px; display: none">
@inproceedings{bei2021learning,
  title={Learning Semantic-Aware Dynamics for Video Prediction},
  author={Bei, Xinzhu and Yang, Yanchao and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={902--912},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- Learning2Manipulate -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/learning2manipulate.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">Learning to Manipulate Individual Objects in an Image</a></b><br><br><b>Yanchao Yang*</b>, Yutong Chen*, Stefano Soatto<br><br> CVPR 2020 <br><br>
<a href="https://arxiv.org/abs/2004.05495">arXiv</a>/<a href="https://github.com/ChenYutongTHU/Learning-to-manipulate-individual-objects-in-an-image-Implementation">Code</a>/<a href="https://www.youtube.com/watch?v=EHQZDwakiio">Video</a>/<a href="javascript:hideshow(document.getElementById('learning2manipulate'))">bibtex</a>
</p><pre><p id="learning2manipulate" style="font:18px; display: none">
@inproceedings{yang2020learning,
  title={Learning to manipulate individual objects in an image},
  author={Yang, Yanchao and Chen, Yutong and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6558--6567},
  year={2020}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- FDA -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/fda.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">FDA: Fourier Domain Adaptation for Semantic Segmentation</a></b><br><br><b>Yanchao Yang</b>, Stefano Soatto<br><br> CVPR 2020 <br><br>
<a href="https://arxiv.org/abs/2004.05498">arXiv</a>/<a href="https://github.com/YanchaoYang/FDA">Code</a>/<a href="https://www.youtube.com/watch?v=MyQPj5DATc4">Video</a>/<a href="javascript:hideshow(document.getElementById('fda'))">bibtex</a>
</p><pre><p id="fda" style="font:18px; display: none">
@inproceedings{yang2020fda,
  title={Fda: Fourier domain adaptation for semantic segmentation},
  author={Yang, Yanchao and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4085--4095},
  year={2020}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- PCEDA -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/pceda.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">Phase Consistent Ecological Domain Adaptation</a></b><br><br><b>Yanchao Yang*</b>, Dong Lao*, Ganesh Sundaramoorthi, Stefano Soatto<br><br> CVPR 2020 <br><br>
<a href="https://arxiv.org/abs/2004.04923">arXiv</a>/<a href="https://github.com/donglao/PCEDA">Code</a>/<a href="https://www.youtube.com/watch?v=WRTpofPGeKA">Video</a>/<a href="javascript:hideshow(document.getElementById('pceda'))">bibtex</a>
</p><pre><p id="pceda" style="font:18px; display: none">
@inproceedings{yang2020phase,
  title={Phase consistent ecological domain adaptation},
  author={Yang, Yanchao and Lao, Dong and Sundaramoorthi, Ganesh and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9011--9020},
  year={2020}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- DDP -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/ddp.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">Dense Depth Posterior (DDP) from Single Image and Sparse Range</a></b><br><br><b>Yanchao Yang</b>, Alex Wong, Stefano Soatto<br><br> CVPR 2019 <br><br>
<a href="https://arxiv.org/abs/1901.10034">arXiv</a>/<a href="">Code</a>/<a href="javascript:hideshow(document.getElementById('ddp'))">bibtex</a>
</p><pre><p id="ddp" style="font:18px; display: none">
@inproceedings{yang2019dense,
  title={Dense depth posterior (ddp) from single image and sparse range},
  author={Yang, Yanchao and Wong, Alex and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3353--3362},
  year={2019}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- CIS -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/cis.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">Unsupervised Moving Object Detection via Contextual Information Separation</a></b><br><br><b>Yanchao Yang*</b>, Antonio Loquercio*, Davide Scaramuzza, Stefano Soatto<br><br> CVPR 2019 <br><br>
<a href="https://arxiv.org/abs/1901.03360">arXiv</a>/<a href="https://github.com/antonilo/unsupervised_detection">Code</a>/<a href="javascript:hideshow(document.getElementById('cis'))">bibtex</a>
</p><pre><p id="cis" style="font:18px; display: none">
@inproceedings{yang2019unsupervised,
  title={Unsupervised moving object detection via contextual information separation},
  author={Yang, Yanchao and Loquercio, Antonio and Scaramuzza, Davide and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={879--888},
  year={2019}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- IRISS -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/iriss.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">Intraocular robotic interventional surgical system (IRISS): semi-automated OCT-guided cataract removal</a></b><br><br>Cheng-Wei Chen, Yu-Hsiu Lee, Matthew J Gerber, Harrison Cheng, <b>Yanchao Yang</b>, Andrea Govetto, Anibal Andres Francone, Stefano Soatto, Warren S Grundfest, Jean-Pierre Hubschman, Tsu-Chin Tsao<br><br> The International Journal of Medical Robotics and Computer Assisted Surgery <br><br>
<a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/rcs.1949">PDF</a>/<a href="javascript:hideshow(document.getElementById('iriss'))">bibtex</a>
</p><pre><p id="iriss" style="font:18px; display: none">
@article{chen2018intraocular,
  title={Intraocular robotic interventional surgical system (IRISS): semi-automated OCT-guided cataract removal},
  author={Chen, Cheng-Wei and Lee, Yu-Hsiu and Gerber, Matthew J and Cheng, Harrison and Yang, Yan-Chao and Govetto, Andrea and Francone, Anibal Andr{\'e}s and Soatto, Stefano and Grundfest, Warren S and Hubschman, Jean-Pierre and others},
  journal={The International Journal of Medical Robotics and Computer Assisted Surgery},
  volume={14},
  number={6},
  pages={e1949},
  year={2018},
  publisher={Wiley Online Library}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- CPN -->
<tbody><tr>
<td width="30%" valign="middle"><p><img src="./yanchaoy/cpn.png" width="360" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="middle"><p>
    <b><a href="">Conditional Prior Networks for Optical Flow</a></b><br><br><b>Yanchao Yang</b>, Stefano Soatto<br><br> ECCV 2018 <br><br>
<a href="https://arxiv.org/abs/1807.10378">arXiv</a>/<a href="https://github.com/YanchaoYang/Conditional-Prior-Networks">Code</a>/<a href="javascript:hideshow(document.getElementById('cpn'))">bibtex</a>
</p><pre><p id="cpn" style="font:18px; display: none">
@inproceedings{yang2018conditional,
  title={Conditional prior networks for optical flow},
  author={Yang, Yanchao and Soatto, Stefano},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={271--287},
  year={2018}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>

</tbody></table>

</ul>
</div>
<br>

<!-- <h3>Preprints</h3>
<ul>


<table width="100%">




</tbody></table>


</ul>
</div>
<br>-->



<a name="education"></a>
<div class="section">
<h3>Education</h3>
<ul>
<li>2014.9 - 2019.4: Ph.D. in Computer Science, University of California, Los Angeles (UCLA)</li>
<li>2011.9 - 2014.6: M.S. in Electrical Engineering, King Abdullah University of Science and Technology (KAUST)</li>
<li>2007.9 - 2011.7: B.E. in Electronic Information Engineering, University of Science and Technology of China (USTC)</li>
</ul>
</div>
<br>


<a name="service"></a>
<div class="section">
<h3>Professional Service</h3>
<ul>
    <li> Program committee/reviewer:
    <ul>
        <li> Conferences: CVPR, ICCV, ECCV, ICML, ICLR, IJCAI, AAAI, NeurIPS</li>
        <li> Journals: TPAMI, TIP, IJCV, RAL</li>
    </ul>
</div>



<hr>
<div id="footer" style="font-size:10">Yanchao Yang <i>Last updated: September 30, 2021</i></div>

</body></html>
