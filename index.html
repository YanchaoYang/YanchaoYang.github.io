<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<title>Yanchao Yang</title>
<link rel="stylesheet" type="text/css" href="./yanchaoy/main.css">
</head>

<head>
    <style>
        p {
            margin-left: 10px;
        }
    </style>
</head>

<head>
    <style>
        pl {
            margin-left: 18px;
        }
    </style>
</head>

<head>
    <style>
        mark {
            background-color: yellow;
            color: black;
        }
    </style>
</head>


<body>

<p>
<table>
<tbody><tr>
<td></td>
<td><img src="./yanchaoy/yanchao_pic.jpg" width="205"></td>
<td>
<div style="font-size:34; font-weight:bold">Yanchao Yang</div>
<div>
Assistant Professor<br>
<a href="https://www.eee.hku.hk/">Electrical and Electronic Engineering</a> and the <a href="https://datascience.hku.hk/">Institute of Data Science</a><br>
<a href="https://www.hku.hk/">The University of Hong Kong</a><br>
<br>

<b>Email:</b> <tt>yanchaoy at hku dot hk</tt><br>
<b>Office:</b> P307C, Graduate House, HKU <br>
<a href="https://scholar.google.com/citations?user=r2tKnV4AAAAJ&hl=en">[Google Scholar]</a><a href="https://github.com/YanchaoYang">[Github]</a><a href="https://www.linkedin.com/in/yanchao-yang-b95b13ba/">[LinkedIn]</a>

</div>
</td>
</tr>
</tbody></table>
</p>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>


<div class="section">
<h3><pl>About Me</pl></h3>
<ul>
I am an Assistant Professor at HKU, jointly appointed by the Department of <a href="https://www.eee.hku.hk/">Electrical and Electronic Engineering (EEE)</a> and the HKU Musketeers Foundation <a href="https://datascience.hku.hk/">Institute of Data Science (HKU-IDS)</a>. 
I was a Postdoctoral Research Fellow at <a href="https://www.stanford.edu/">Stanford University</a> with <a href="https://geometry.stanford.edu/member/guibas/index.html">Leonidas J. Guibas</a> and received my Ph.D. from the <a href="https://www.ucla.edu/">University of California, Los Angeles (UCLA)</a> with <a href="http://web.cs.ucla.edu/~soatto/">Stefano Soatto</a>.
Earlier, I obtained my Master's and Bachelor's degrees from <a href="https://www.kaust.edu.sa/en">KAUST</a> and <a href="https://en.ustc.edu.cn/">USTC</a>, respectively.
<br>
<br>
I do research in computer vision, machine learning, and robotics, and I am interested in self-supervised and semi-supervised techniques that allow autonomous agents to learn at low-annotation regimes. My long-term goal is to design learning algorithms that enable autonomous agents to continuously acquire object-centric representations through active perception with multimodal signals for compositional scene understanding and reasoning. My recent effort is to leverage information-theoretic tools to revolutionize scene representations for embodied intelligence.
<br>
<br>
<mark>[hiring for Ph.D. and Postdocs!]</mark> We are constantly looking for talents with the motivation and responsibility to build fundamentals for autonomous agents to learn from unlimited data streams toward intelligent interactions with the physical world. This quest is related but not limited to computer vision and graphics, machine learning, communication and information theory, multimodal data mining, robotics, and human-machine interaction.
<mark>Please contact me via email if you are interested in our research or potential collaborations. Students can choose either EEE or Data Science when applying, but make sure to notify me after the submission.</mark>
</div>



<div class="section">
<h3><pl>News</pl></h3>
<ul>
<li> [10/2022] Our workshop on Visual Object-oriented Learning meets Interaction (<a href="https://geometry.stanford.edu/voli/">VOLI</a>) is successfully held at ECCV 2022.
</ul>
</div>



<div class="section">
<h3><pl>Students</pl></h3>
<ul>
<li> Xiaomeng Xu (Tsinghua University)
<li> Ruohan Zhang (USTC)
<li> Ruoxi Shi (Shanghai Jiao Tong University)
<li> Yang Zheng (Tsinghua University, now at Stanford)
<li> Qi Li (Tsinghua University, now at UCLA)
<li> Chuanyu Pan (Tsinghua University, now at UC Berkeley)
<li> Brian Lai (UCLA, now at Aurora)
<li> Yutong Chen (Tsinghua University)
</ul>
</div>
<br>



<div class="mainsection">

<a name="publications"></a>
<div class="mainsection">
<h3>Publications</h3>
<ul>
See <a href="https://scholar.google.com/citations?user=r2tKnV4AAAAJ&hl=en">Google Scholar</a> for a full list of papers. *: equivalent contribution, <span>&#8224;</span>: corresponding author<br><br>
<table width="100%">





<!-- GIMO -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/GIMO.jpg" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">GIMO: Gaze-Informed Human Motion Prediction in Context</a></b><br><br>Yang Zheng, <b>Yanchao Yang</b><span>&#8224;</span>, Kaichun Mo, Jiaman Li, Tao Yu, Yebin Liu, C. Karen Liu, Leonidas J. Guibas<br><br> ECCV 2022 <br><br>
<a href="https://arxiv.org/abs/2204.09443">arXiv</a>/<a href="https://github.com/y-zheng18/gimo">code</a>/<a href="https://geometry.stanford.edu/projects/gimo/">project page</a>/<a href="javascript:hideshow(document.getElementById('GIMO'))">bibtex</a>
</p><pre><p id="GIMO" style="font:18px; display: none">
@article{zheng2022gimo,
  title={GIMO: Gaze-Informed Human Motion Prediction in Context},
  author={Zheng, Yang and Yang, Yanchao and Mo, Kaichun and Li, Jiaman and Yu, Tao and Liu, Yebin and Liu, Karen and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:2204.09443},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- SpOT -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/SpOT.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">SpOT: Spatiotemporal Modeling for 3D Object Tracking</a></b><br><br>Colton Stearns, Davis Rempe, Jie Li, Rares Ambrus, Sergey Zakharov, Vitor Guizilini, <b>Yanchao Yang</b>, Leonidas J. Guibas<br><br> ECCV 2022 (Oral) <br><br>
<a href="https://arxiv.org/abs/2207.05856">arXiv</a>/<a href="javascript:hideshow(document.getElementById('SpOT'))">bibtex</a>
</p><pre><p id="SpOT" style="font:18px; display: none">
@article{stearns2022spot,
  title={SpOT: Spatiotemporal Modeling for 3D Object Tracking},
  author={Stearns, Colton and Rempe, Davis and Li, Jie and Ambrus, Rares and Zakharov, Sergey and Guizilini, Vitor and Yang, Yanchao and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:2207.05856},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- DAGAI -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/DAGAI.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">Domain Adaptation on Point Clouds via Geometry-Aware Implicits</a></b><br><br>Yuefan Shen*, <b>Yanchao Yang</b>*<span>&#8224;</span>, Mi Yan, He Wang, Youyi Zheng<span>&#8224;</span>, Leonidas J. Guibas<br><br> CVPR 2022 <br><br>
<a href="https://arxiv.org/abs/2112.09343">arXiv</a>/<a href="https://github.com/Jhonve/ImplicitPCDA">code</a>/<a href="javascript:hideshow(document.getElementById('DAGAI'))">bibtex</a>
</p><pre><p id="DAGAI" style="font:18px; display: none">
@article{shen2021domain,
  title={Domain Adaptation on Point Clouds via Geometry-Aware Implicits},
  author={Shen, Yuefan and Yang, Yanchao and Yan, Mi and Wang, He and Zheng, Youyi and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2112.09343},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


    
<!-- ADeLA -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/adela.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation</a></b><br><br><b>Yanchao Yang</b>*<span>&#8224;</span>, Hanxiang Ren*, He Wang, Bokui Shen, Qingnan Fan, Youyi Zheng<span>&#8224;</span>, C. Karen Liu, Leonidas J. Guibas<br><br> CVPR 2022 (Oral)<br><br>
<a href="https://arxiv.org/abs/2107.14285">arXiv</a>/<a href="https://github.com/ReNginx/ADeLA">code</a>/<a href="javascript:hideshow(document.getElementById('adela'))">bibtex</a>
</p><pre><p id="adela" style="font:18px; display: none">
@article{yang2021adela,
  title={ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation},
  author={Yang, Yanchao and Ren, Hanxiang and Wang, He and Shen, Bokui and Fan, Qingnan and Zheng, Youyi and Liu, C Karen and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2107.14285},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
    
    
    
<!-- ObjectP -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/objectP.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">Object Pursuit: Building a Space of Objects via Discriminative Weight Generation</a></b><br><br>Chuanyu Pan*, <b>Yanchao Yang</b>*, Kaichun Mo, Yueqi Duan, Leonidas J. Guibas<br><br> ICLR 2022 <br><br>
<a href="https://arxiv.org/abs/2112.07954">arXiv</a>/<a href="https://github.com/pptrick/Object-Pursuit">code</a>/<a href="https://pptrick.github.io/static/object-pursuit/index.html">project page</a>/<a href="javascript:hideshow(document.getElementById('ObjectP'))">bibtex</a>
</p><pre><p id="ObjectP" style="font:18px; display: none">
@article{pan2021object,
  title={Object Pursuit: Building a Space of Objects via Discriminative Weight Generation},
  author={Pan, Chuanyu and Yang, Yanchao and Mo, Kaichun and Duan, Yueqi and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2112.07954},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>



<!-- IFR-Explore -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/IFR_explore.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes</a></b><br><br>Qi Li*, Kaichun Mo*, <b>Yanchao Yang</b>, Hang Zhao, Leonidas J. Guibas<br><br> ICLR 2022 <br><br>
<a href="https://arxiv.org/abs/2112.05298">arXiv</a>/<a href="https://github.com/liqi17thu/IFR_explore">code</a>/<a href="javascript:hideshow(document.getElementById('IFR-Explore'))">bibtex</a>
</p><pre><p id="IFR-Explore" style="font:18px; display: none">
@article{li2021ifr,
  title={IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes},
  author={Li, Qi and Mo, Kaichun and Yang, Yanchao and Zhao, Hang and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2112.05298},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>



<!-- DCL -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/dcl.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis</a></b><br><br><b>Yanchao Yang</b>*<span>&#8224;</span>, Yuefan Shen*, Youyi Zheng<span>&#8224;</span>, C. Karen Liu, Leonidas J. Guibas<br><br> RA-L & ICRA 2022 <br><br>
<a href="https://arxiv.org/abs/2107.13087">arXiv</a>/<a href="https://github.com/Jhonve/DCL-DepthSynthesis">code</a>/<a href="javascript:hideshow(document.getElementById('dcl'))">bibtex</a>
</p><pre><p id="dcl" style="font:18px; display: none">
@article{yang2021dcl,
  title={DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis},
  author={Yang, Yanchao and Shen, Yuefan and Zheng, Youyi and Liu, C Karen and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2107.13087},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- DyStaB -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/dystab.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">DyStaB: Unsupervised Object Segmentation via Dynamic-Static Bootstrapping</a></b><br><br><b>Yanchao Yang</b>*, Brian Lai*, Stefano Soatto<br><br> CVPR 2021 (Oral) <br><br>
<a href="https://arxiv.org/abs/2008.07012">arXiv</a>/<a href="https://github.com/blai88/unsupervised_segmentation">Code</a>/<a href="https://www.youtube.com/watch?v=jJM6DVfWNEs">Video</a>/<a href="javascript:hideshow(document.getElementById('dystab'))">bibtex</a>
</p><pre><p id="dystab" style="font:18px; display: none">
@inproceedings{yang2021dystab,
  title={DyStaB: Unsupervised Object Segmentation via Dynamic-Static Bootstrapping},
  author={Yang, Yanchao and Lai, Brian and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2826--2836},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- SADM -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/sadm.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">Learning Semantic-Aware Dynamics for Video Prediction</a></b><br><br>Xinzhu Bei, <b>Yanchao Yang</b>, Stefano Soatto<br><br> CVPR 2021 <br><br>
<a href="https://arxiv.org/abs/2104.09762">arXiv</a>/<a href="javascript:hideshow(document.getElementById('sadm'))">bibtex</a>
</p><pre><p id="sadm" style="font:18px; display: none">
@inproceedings{bei2021learning,
  title={Learning Semantic-Aware Dynamics for Video Prediction},
  author={Bei, Xinzhu and Yang, Yanchao and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={902--912},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- Learning2Manipulate -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/learning2manipulate.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">Learning to Manipulate Individual Objects in an Image</a></b><br><br><b>Yanchao Yang</b>*, Yutong Chen*, Stefano Soatto<br><br> CVPR 2020 <br><br>
<a href="https://arxiv.org/abs/2004.05495">arXiv</a>/<a href="https://github.com/ChenYutongTHU/Learning-to-manipulate-individual-objects-in-an-image-Implementation">Code</a>/<a href="https://www.youtube.com/watch?v=EHQZDwakiio">Video</a>/<a href="javascript:hideshow(document.getElementById('learning2manipulate'))">bibtex</a>
</p><pre><p id="learning2manipulate" style="font:18px; display: none">
@inproceedings{yang2020learning,
  title={Learning to manipulate individual objects in an image},
  author={Yang, Yanchao and Chen, Yutong and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6558--6567},
  year={2020}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- FDA -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/fda.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">FDA: Fourier Domain Adaptation for Semantic Segmentation</a></b><br><br><b>Yanchao Yang</b>, Stefano Soatto<br><br> CVPR 2020 <br><br>
<a href="https://arxiv.org/abs/2004.05498">arXiv</a>/<a href="https://github.com/YanchaoYang/FDA">Code</a>/<a href="https://www.youtube.com/watch?v=MyQPj5DATc4">Video</a>/<a href="javascript:hideshow(document.getElementById('fda'))">bibtex</a>
</p><pre><p id="fda" style="font:18px; display: none">
@inproceedings{yang2020fda,
  title={Fda: Fourier domain adaptation for semantic segmentation},
  author={Yang, Yanchao and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4085--4095},
  year={2020}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- PCEDA -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/pceda.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">Phase Consistent Ecological Domain Adaptation</a></b><br><br><b>Yanchao Yang</b>*, Dong Lao*, Ganesh Sundaramoorthi, Stefano Soatto<br><br> CVPR 2020 <br><br>
<a href="https://arxiv.org/abs/2004.04923">arXiv</a>/<a href="https://github.com/donglao/PCEDA">Code</a>/<a href="https://www.youtube.com/watch?v=WRTpofPGeKA">Video</a>/<a href="javascript:hideshow(document.getElementById('pceda'))">bibtex</a>
</p><pre><p id="pceda" style="font:18px; display: none">
@inproceedings{yang2020phase,
  title={Phase consistent ecological domain adaptation},
  author={Yang, Yanchao and Lao, Dong and Sundaramoorthi, Ganesh and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9011--9020},
  year={2020}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- DDP -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/ddp.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">Dense Depth Posterior (DDP) from Single Image and Sparse Range</a></b><br><br><b>Yanchao Yang</b>, Alex Wong, Stefano Soatto<br><br> CVPR 2019 <br><br>
<a href="https://arxiv.org/abs/1901.10034">arXiv</a>/<a href="">Code</a>/<a href="javascript:hideshow(document.getElementById('ddp'))">bibtex</a>
</p><pre><p id="ddp" style="font:18px; display: none">
@inproceedings{yang2019dense,
  title={Dense depth posterior (ddp) from single image and sparse range},
  author={Yang, Yanchao and Wong, Alex and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3353--3362},
  year={2019}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- CIS -->
<tbody><tr>
<td width="38%" valign="middle"><p><img src="./yanchaoy/cis.png" width="450" alt="" style="border-style: none" align="top"></p></td>
<td width="62%" valign="middle"><p>
    <b><a href="">Unsupervised Moving Object Detection via Contextual Information Separation</a></b><br><br><b>Yanchao Yang</b>*, Antonio Loquercio*, Davide Scaramuzza, Stefano Soatto<br><br> CVPR 2019 <br><br>
<a href="https://arxiv.org/abs/1901.03360">arXiv</a>/<a href="https://github.com/antonilo/unsupervised_detection">Code</a>/<a href="javascript:hideshow(document.getElementById('cis'))">bibtex</a>
</p><pre><p id="cis" style="font:18px; display: none">
@inproceedings{yang2019unsupervised,
  title={Unsupervised moving object detection via contextual information separation},
  author={Yang, Yanchao and Loquercio, Antonio and Scaramuzza, Davide and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={879--888},
  year={2019}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>




</tbody></table>

</ul>
</div>
<br>

<!-- <h3>Preprints</h3>
<ul>
<table width="100%">
</tbody></table>
</ul>
</div>
<br>-->



<a name="education"></a>
<div class="section">
<h3>Education</h3>
<ul>
<li>2014.9 - 2019.4: Ph.D. in Computer Science, University of California, Los Angeles (UCLA)</li>
<li>2011.9 - 2014.6: M.S. in Electrical Engineering, King Abdullah University of Science and Technology (KAUST)</li>
<li>2007.9 - 2011.7: B.E. in Electronic Information Engineering, University of Science and Technology of China (USTC)</li>
</ul>
</div>
<br>


<a name="service"></a>
<div class="section">
<h3>Professional Service</h3>
<ul>
    <li> Program committee/reviewer:
    <ul>
        <li> Conferences: CVPR, ICCV, ECCV, ICML, ICLR, IJCAI, AAAI, NeurIPS</li>
        <li> Journals: TPAMI, TIP, IJCV, RAL</li>
    </ul>
</div>



<hr>
<div id="footer" style="font-size:10">Yanchao Yang <i>Last updated: October, 2022</i></div>

</body></html>
